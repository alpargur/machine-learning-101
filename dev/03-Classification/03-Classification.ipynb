{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Classification\n",
    "\n",
    "The goal of this exercise is to to develop an understanding how to train a binary classifier and how to mesaure its performance with different performance metrics.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "To solve this notebook you need the knowledge from the previous notebook. If you have problems solving it, take another look at last week's notebook.\n",
    "    \n",
    "It's also recommended to read the chapter 3 of the book in advance.\n",
    "</div>\n",
    "\n",
    "**Task**: In this exercise, we want to use pictures of banknotes to identify whether they are forged or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Run this cell two import the following modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"left\">Banknote Authentication Data Set</h2>\n",
    "\n",
    "For this we use a public available dataset called [_banknote authentication Data Set_](https://archive.ics.uci.edu/ml/datasets/banknote+authentication#). This dataset was created by two resarchers of the University of Applied Sciences in Ostwestfalen-Lippe. They took 1372 images of genuine and forged banknote-like specimens with an industrial camera usually used for print inspection and applied a Welvelt Transformation on them. Then they extracted 4 features of the images:\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous)\n",
    "2. skewness of Wavelet Transformed image (continuous)\n",
    "3. curtosis of Wavelet Transformed image (continuous)\n",
    "4. entropy of image (continuous)\n",
    "\n",
    "The last column shows if the banknote is valid or fake:\n",
    "\n",
    "- class 0 is genuine / authentic\n",
    "- class 1 is fake / forgery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset/data_banknote_authentication.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br> \n",
    "Find a way to count how many fake and how many real banknotes are in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"left\">Data Preprocessing</h2>\n",
    "\n",
    "### Train Test Split\n",
    "\n",
    "After we inspect the data, we can split our data in a test set and training set. Therefore we use the built in function of SciKit-learn `train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Use the train_test_split function, to split up dataset. The test set should consists of 20% of the total data data. Save the results in the variables X_train, X_test, y_train and y_test. Set the variable random_state = 42.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data\n",
    "\n",
    "Because the data is given is different units, we must scale it. Therefore we use the built-in class `StandardScaler` of SciKit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Scale the training and test set with the StandardScaler sc. Save the results in the variables X_train_scaled and X_test_scaled.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_scaled, X_test_scaled = [], []\n",
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"left\">Train and Evaluate your Model</h2>\n",
    "\n",
    "Now that we've done with the preprocessing, we can start to train and evaluate your model. In this exercices, we will use the SGDClassifier of SciKit-learn. By default, this functions fits a linear support vector machine (SVM) with stochastic gradient descent (SGD) learning. These terms will be discussed in the next lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with Cross Validation\n",
    "\n",
    "After we have fit your model to the data, we can start to evaluate it. For this we use the cross validation score, like in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Evaluate the model, using the cross_val_score function of SciKit-learn. Make a 3-Fold cross validation using your training set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, all these predictions are more than 97% correct. In fact, that we have nearly a equally number of fake and real banknotes in your training set, this is a really good result. But as we've learned in the book, there are more metrics to measure the performance of your model.\n",
    "\n",
    "### Make some Predictions\n",
    "\n",
    "To apply this metrics to your model, we need to make some predictions and save them for further investigations. Therefore we use the function `cross_val_predict` of SciKit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Use the function cross_val_predict to predict the data of your training set using a 3 folds. Save the result in the variable y_train_pred.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = []\n",
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "When have the predicted values `y_train_predict`, we can compare them with the truth values `y_train` and plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickLabels = ['authentic', 'fake']\n",
    "ax = sns.heatmap(confusion_matrix(y_train, y_train_pred), annot=True, fmt=\"d\", yticklabels=tickLabels, xticklabels=tickLabels)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix has the following notation.\n",
    "\n",
    "$ \\text{confusion matrix} =\n",
    "\\begin{pmatrix}\n",
    "TN & FP \\\\\n",
    "FN & TP\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "On the upper left, we have the True Negatives (_TN_). That are the values that were correctly predicted as negatives. Because negative ($0$) is the class _authentic_, we can see the number of authentic banknotes, that are correctly predicted as authentic. To the right are the False Positives (_FP_). These are the authentic banknotes that were wrongly labeled as positives ($1$), i.e. fake.\n",
    "\n",
    "On the lower left, you can see the False Negatives, i.e. the fake banknotes that are wrongly predicted as authentic and on the lower right, there are the number of fake banknotes, that are correctly predicted as fake (_TP_).\n",
    "\n",
    "All in all we have a very good prediction, because on the diagonal are only large numbers and only 20 predictions are wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presicion and Recall\n",
    "\n",
    "The precision and recall are another metric to measure to performance of your model. They can be directly computed from the values in the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "The precision describes what proportion of positive identifications was actually correct.\n",
    "\n",
    "It can be computed as follows:\n",
    "\n",
    "$\\text{precision} = \\frac{TP}{TP+FP}$\n",
    "\n",
    "You can also use the built-in function of Scikit-learn `precision_score`.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Use the function precision_score to calculate the precision of your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "The recall describes what proportion of actual positives was identified correctly.\n",
    "\n",
    "It can be computed as follows:\n",
    "\n",
    "$\\text{recall} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "You can also use the built-in function of Scikit-learn `recall_score`.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Use the function recall_scare to calculate the recall of your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score score is the harmonic mean of the precision and recall. SciKit-learn has a built-it in function for this, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Adjustments to your Model\n",
    "\n",
    "The goal of your classifier is to not accept any fake banknotes. Because of the Precision/Recall Trade-off this means, that we will mark some real banknotes as fake. Since the fake banknotes are labeled with $1$, we don't want to have false negatives (_FN_), so we want high _Recall_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Use the function precision_recall_curve to find a threshold, so that your model won't accept any fake banknotes. Save the threshold in the variable threshhold_highest_recall. If you are not sure, how to do it, have another look at the third chapter of the book.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold_highest_recall = 0\n",
    "y_scores = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3, method=\"decision_function\")\n",
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this threshhold to make new predictions of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_recall = (y_scores >= threshhold_highest_recall)\n",
    "confusion_matrix(y_train, y_pred_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, as we can see, there are no False Negatives any more. Great! \n",
    "\n",
    "But there also more False Positives, so we marked 26 authentic banknotes as fake. This leads to a smaller precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_pred_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-Curve and AUC-Score\n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "- True Positive Rate (TPR), where $\\text{TPR} = \\frac{TP}{TP+FN}$, also known als recall and\n",
    "- False Positive Rate (FPR), where $\\text{FPR} = \\frac{FP}{FP+TN}$.\n",
    "\n",
    "SciKit-learn provides a build-in function called `roc_curve` to compute the rates at the different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1.01])  \n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)  \n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))                    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a really perfect course of the curve, because we are very far in the upper left corner. The Area under the curve is called AUC_score can the computed with the roc_auc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Use the function roc_auc_score to compute the AUC-Score.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally test your model with the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Task</b><br>\n",
    "Evaluate accuracy, precision and recall for the test set. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Code Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
